{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using Manual Features and Traditional Machine Learning Models\n",
    "These examples are taken from the NLTK Manual\n",
    "https://www.nltk.org/book/ch06.html\n",
    "\n",
    "In this example, we will extract manual features such as the first and last letter of a name and use them in a Naive Bayes classifier.\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "First, load the names dataset and prepare the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('Lissie', 'female')\n('Danya', 'female')\n('Welby', 'male')\n('Antone', 'male')\n('Bert', 'female')\n('Erek', 'male')\n('Gayleen', 'female')\n('Rory', 'female')\n('Donelle', 'female')\n('Felicle', 'female')\n[nltk_data] Downloading package names to /home/sebastian/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n"
    }
   ],
   "source": [
    "# Load the data\n",
    "from nltk.corpus import names\n",
    "nltk.download(\"names\") # Only need to run this once\n",
    "\n",
    "# Annotate the data based on the text file they come from (male vs. female)\n",
    "labeled_names = ([(name, \"male\") for name in names.words(\"male.txt\")] + \n",
    "                 [(name, \"female\") for name in names.words(\"female.txt\")])\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "for i in range(10):\n",
    "    print(labeled_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from the Text\n",
    "\n",
    "Now let's extract features from the text. We'll use the first and last letter of each name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "({'first_letter': 'l', 'last_letter': 'e'}, 'female')\n({'first_letter': 'd', 'last_letter': 'a'}, 'female')\n({'first_letter': 'w', 'last_letter': 'y'}, 'male')\n({'first_letter': 'a', 'last_letter': 'e'}, 'male')\n({'first_letter': 'b', 'last_letter': 't'}, 'female')\n({'first_letter': 'e', 'last_letter': 'k'}, 'male')\n({'first_letter': 'g', 'last_letter': 'n'}, 'female')\n({'first_letter': 'r', 'last_letter': 'y'}, 'female')\n({'first_letter': 'd', 'last_letter': 'e'}, 'female')\n({'first_letter': 'f', 'last_letter': 'e'}, 'female')\n"
    }
   ],
   "source": [
    "def gender_features(name):\n",
    "    \"\"\" Extracts features from the word \"\"\"\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    return features\n",
    "\n",
    "labeled_features = [(gender_features(n), name) for (n, name) in labeled_names]\n",
    "\n",
    "for i in range(10):\n",
    "    print(labeled_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the Features for a Statistical Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Encoded one-hot features\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 1. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 1. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\nShape: (7944, 52)\n\nEncoded labels\n[1 1 0 ... 1 1 1]\nShape: (7944,)\n"
    }
   ],
   "source": [
    "# Convert the features to one-hot vectors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X = [[f[\"first_letter\"], f[\"last_letter\"]] for (f, _) in labeled_features]\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Labels are a binary output: 0 for male, 1 for female\n",
    "y = np.array([0 if gnd==\"male\" else 1 for (_, gnd) in labeled_features])\n",
    "names = [n for (n, _) in labeled_names]\n",
    "\n",
    "print(\"Encoded one-hot features\")\n",
    "print(X_encoded)\n",
    "print(\"Shape: {}\".format(X_encoded.shape))\n",
    "print(\"\")\n",
    "print(\"Encoded labels\")\n",
    "print(y_encoded)\n",
    "print(\"Shape: {}\".format(y_encoded.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Fit, and Evaluate a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Accuracy: 3908/5000 = 78.16%\nTest Accuracy: 2266/2944 = 76.97%\n\nExample predictions\nName: Raleigh\tActual: male\tPredicted: male\nName: Dora\tActual: female\tPredicted: female\nName: Amie\tActual: female\tPredicted: female\nName: Norton\tActual: male\tPredicted: male\nName: Helyn\tActual: female\tPredicted: male\nName: Tiertza\tActual: female\tPredicted: female\nName: Ginnifer\tActual: female\tPredicted: male\nName: Cheri\tActual: female\tPredicted: female\nName: Carson\tActual: male\tPredicted: female\nName: Nico\tActual: male\tPredicted: male\n"
    }
   ],
   "source": [
    "# Split into training and test, remember we already shuffled\n",
    "cutoff_idx = 5000\n",
    "X_train = X_encoded[:cutoff_idx]\n",
    "y_train = y[:cutoff_idx]\n",
    "names_train = names[:cutoff_idx]\n",
    "X_test  = X_encoded[cutoff_idx:]\n",
    "y_test  = y[cutoff_idx:]\n",
    "names_test  = names[cutoff_idx:]\n",
    "\n",
    "# Create a model and fit\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "model = CategoricalNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and test sets\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "num_train = len(y_train)\n",
    "num_correct_train = np.sum(y_train == y_pred_train) \n",
    "train_acc =  100 * num_correct_train / num_train\n",
    "print(\"Training Accuracy: {}/{} = {:.2f}%\".format(\n",
    "    num_correct_train, num_train, train_acc))\n",
    "num_test = len(y_test)\n",
    "num_correct_test = np.sum(y_test == y_pred_test) \n",
    "test_acc = 100 * num_correct_test / num_test\n",
    "print(\"Test Accuracy: {}/{} = {:.2f}%\".format(\n",
    "    num_correct_test, num_test, test_acc))\n",
    "\n",
    "# Print a few examples\n",
    "print(\"\\nExample predictions\")\n",
    "for i in np.random.randint(len(y_test), size=10):\n",
    "    actual = \"male\" if y_test[i]==0 else \"female\"\n",
    "    pred = \"male\" if y_pred[i]==0 else \"female\" \n",
    "    print(\"Name: {}\\tActual: {}\\tPredicted: {}\".format(\n",
    "        names_test[i], actual, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitintronlpconda61e1afdce515499c9770b8779f7d77e0",
   "display_name": "Python 3.6.10 64-bit ('intro-nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}