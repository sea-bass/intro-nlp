{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers\n",
    "\n",
    "In this example, we will be exploring transformer networks through some of the functionality in the [HuggingFace Transformers]( https://github.com/huggingface/transformers) library.\n",
    "\n",
    "This library has great documentation, so please refer to that for more detailed usage after browsing through this simple notebook.\n",
    "\n",
    "To download the library, use `pip` inside your `conda` environment since the `conda-forge` version is quite out of date:\n",
    "\n",
    "```bash\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Usage Example\n",
    "\n",
    "HuggingFace Transformers contains several popular transformer network architectures and pretrained weight checkpoints. The idea is to either use these networks as-is (for example, as pretrained encoders for your own downstream neural network), or a starting points to train on your own datasets.\n",
    "\n",
    "This library supports both PyTorch and TensorFlow 2.0 frameworks, and outputs all transformer networks as trainable networks you can use however you want.\n",
    "\n",
    "Below we will get a [Bidirectional Encoder Representations from Transformers (BERT)](https://github.com/google-research/bert) model, which is popularly as an encoder for machine learning tasks (e.g. question answering) due to its powerful ability to encode words in the context they appear rather than as standalone words.\n",
    "\n",
    "**Historical note:** Before transformers, [Embeddings from Language Models (ELMo)](https://allennlp.org/elmo) did the same thing but using a bidirectional RNN with LSTM units... so it was natural and clever for Google to choose BERT as their catchy acronym. This is also why you will often hear the facetious comment about \"muppets and transformers\" dominating recent machine learning. For more information, check out this blog by [Jay Alammar](http://jalammar.github.io/illustrated-bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: Machine learning is the study of computer algorithms that improve automatically through experience.\n",
      "Hidden states: tensor([[[ 0.0918, -0.6651, -0.5920,  ...,  0.2772, -0.4451,  0.5621],\n",
      "         [ 0.3746,  0.4496, -0.4746,  ...,  0.2892,  0.6671,  0.3278],\n",
      "         [-0.1143,  0.4511, -0.6710,  ..., -0.5707, -0.0513,  0.6873],\n",
      "         ...,\n",
      "         [-0.1761, -0.0033,  0.0998,  ..., -0.9533, -0.9809,  0.2631],\n",
      "         [ 0.8583,  0.0425, -0.4340,  ...,  0.0977, -0.7423, -0.0583],\n",
      "         [ 0.6726,  0.0225, -0.1304,  ...,  0.2195, -0.8982, -0.1860]]])\n",
      "Hidden states shape: torch.Size([1, 16, 768])\n",
      "Attentions shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Get a BERT based tokenizer and model\n",
    "# NOTE: The models will take some time to download the first time\n",
    "from transformers import BertModel, BertTokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name,\n",
    "                                          output_hidden_states=True,\n",
    "                                          output_attentions=True)\n",
    "                                          \n",
    "# Extract outputs from a sample sentence\n",
    "sample_text = \"Machine learning is the study of computer algorithms that improve automatically through experience.\"\n",
    "input_ids = torch.tensor([tokenizer.encode(sample_text, add_special_tokens=True)])\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    hidden_states = outputs[0]\n",
    "    attentions = outputs[1]\n",
    "\n",
    "# Print some key values\n",
    "print(\"Sample text: {}\".format(sample_text))\n",
    "print(\"Hidden states: {}\".format(hidden_states))\n",
    "print(\"Hidden states shape: {}\".format(hidden_states.shape))\n",
    "print(\"Attentions shape: {}\".format(attentions.shape))\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Example: Question Answering\n",
    "\n",
    "HuggingFace Transformers also provides `pipelines` for common tasks like classification, translation, and question answering. Below we will test another pretrained BERT model for question answering applications. \n",
    "\n",
    "This model will use different weights that have been refined for question answering using the [Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/).\n",
    "\n",
    "In the documentation, you can find a full list of [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) and [pretrained models](https://huggingface.co/transformers/pretrained_models.html), among many other useful resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 230/230 [00:00<00:00, 80.0kB/s]\n",
      "\n",
      "Question: When was RoboCup founded?\n",
      "Answer: 1996\n",
      "Score: 0.9921742685001611\n",
      "\n",
      "Question: Who is the president of RoboCup?\n",
      "Answer: Peter Stone\n",
      "Score: 0.994986112397072\n",
      "\n",
      "Question: Where was the 2019 RoboCup international competition held?\n",
      "Answer: Sydney, Australia.\n",
      "Score: 0.8968013221865334\n",
      "\n",
      "Question: What is the main goal of RoboCup?\n",
      "Answer: to promote robotics and AI research\n",
      "Score: 0.4689098121865669\n"
     ]
    }
   ],
   "source": [
    "# Define a question answering pipeline\n",
    "# NOTE: The models will take some time to download the first time\n",
    "from transformers import pipeline\n",
    "model_name = \"bert-large-cased-whole-word-masking-finetuned-squad\"\n",
    "nlp_qa = pipeline(\"question-answering\", \n",
    "                  model=model_name, \n",
    "                  tokenizer=model_name,\n",
    "                  framework=\"pt\")\n",
    "\n",
    "# Define the context and a set of questions\n",
    "# The context is the first few paragraphs from the RoboCup Wikipedia page:\n",
    "# https://en.wikipedia.org/wiki/RoboCup\n",
    "context = \"\"\"\n",
    "RoboCup is an annual international robotics competition proposed and founded in 1996 by a group of university professors (including Hiroaki Kitano, Manuela M. Veloso, and Minoru Asada). The aim of the competition is to promote robotics and AI research by offering a publicly appealing – but formidable – challenge. \n",
    "The name RoboCup is a contraction of the competition's full name, \"Robot Soccer World Cup\", but there are many other areas of competition such as \"RoboCupRescue\", \"RoboCup@Home\" and \"RoboCupJunior\". In 2019, the international competition was held in Sydney, Australia. Peter Stone is the current president of RoboCup, and has been since 2019.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was RoboCup founded?\",\n",
    "    \"Who is the president of RoboCup?\",\n",
    "    \"Where was the 2019 RoboCup international competition held?\",\n",
    "    \"What is the main goal of RoboCup?\"\n",
    "]\n",
    "\n",
    "# Loop through each question and display the answer and confidence score\n",
    "for question in questions:\n",
    "    result = nlp_qa(question=question, context=context)\n",
    "    answer = result[\"answer\"]\n",
    "    score = result[\"score\"]\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Question: {}\".format(question))\n",
    "    print(\"Answer: {}\".format(answer))\n",
    "    print(\"Score: {}\".format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('intro-nlp': conda)",
   "language": "python",
   "name": "python361064bitintronlpconda61e1afdce515499c9770b8779f7d77e0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
