{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Using n-grams\n",
    "\n",
    "In this example, we will extract and count n-grams features from documents, and then use them to generate text. \n",
    "\n",
    "The example is based on the [Python for NLP: Developing an Automatic Text Filler using n-grams example by Usman Malik](https://stackabuse.com/python-for-nlp-developing-an-automatic-text-filler-using-n-grams/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import bs4 as bs\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "Below, we will read a set of Wikipedia articles related to robotics and AI (feel free to change it). \n",
    "\n",
    "After some regular expression based preprocessing to convert the data to lowercase alphanumeric with basic punctuation, we can extract features from this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 50476\n",
      "\n",
      "robotics is an interdisciplinary research area at the interface of computer science and engineering. robotics involves design, construction, operation, and use of robots. the goal of robotics is to design intelligent machines that can help and assist humans in their day to day lives and keep everyone safe. robotics draws on the achievement of information engineering, computer engineering, mechanical engineering, electronic engineering and others.robotics develops machines that can substitute for humans and replicate human actions. robots can be used in many situations and for many purposes, but today many are used in dangerous environments including inspection of radioactive materials, bomb detection and deactivation, manufacturing processes, or where humans cannot survive e g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation. robots can take on any form but some are made to resemble humans in appearance. this is said to help in the \n"
     ]
    }
   ],
   "source": [
    "# Read a Wikipedia page and combine all the paragraphs' text\n",
    "articles = [\"Robotics\", \"Artificial_intelligence\", \"Machine_learning\", \"Computer_vision\", \n",
    "            \"Human-robot_interaction\", \"Robotic_sensing\", \"Robotic_sensors\", \"Cyber-physical_system\", \n",
    "            \"Robot_locomotion\", \"Mobile_robot\", \"Robotic_mapping\", \"Robotic_manipulation\"]\n",
    "article_text = \"\"\n",
    "for article_name in articles:\n",
    "    raw_html = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/\" + article_name)\n",
    "    raw_html = raw_html.read()\n",
    "    article_html = bs.BeautifulSoup(raw_html, \"lxml\")\n",
    "    article_paragraphs = article_html.find_all(\"p\")\n",
    "    for para in article_paragraphs:\n",
    "        article_text += para.text\n",
    "\n",
    "# Convert all text to lower case and remove anything besides alphanumerics, some punctuation, and space\n",
    "article_text = article_text.lower()\n",
    "article_text = re.sub(r\"[-]\", \" \", article_text)\n",
    "article_text = re.sub(r\"(?P<n1>[A-Za-z])[^A-za-z ](?P<n2>[A-za-z])\", \"\\g<n1> \\g<n2>\", article_text)\n",
    "article_text = re.sub(r\"[^A-Za-z., ]\", \"\", article_text)\n",
    "\n",
    "# Print the first few words\n",
    "num_words = len(nltk.word_tokenize(article_text))\n",
    "print(\"Total number of words: {}\\n\".format(num_words))\n",
    "print(article_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract n-gram Features\n",
    "\n",
    "Below we will extract n-grams from the documents from scratch.\n",
    "\n",
    "Each unique n-gram (e.g. the trigram \"robotics is an\") is stored as a Python dictionary key, where its values are a list of all the words following that sequence as seen in the entire dataset. In theory, reading an infinitely large dataset will give us perfect probability distribution for generating the next word given any n-gram.\n",
    "\n",
    "However, note that NLTK has several utilities for extracting n-grams, padding sequences, and more. You can see good example from [this notebook on Kaggle by Liling Tan](https://www.kaggle.com/alvations/n-gram-language-model-with-nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3-grams:\n",
      "natural , human: ['esque']\n",
      "university , stanford: ['and']\n",
      "motion estimation ,: ['visual']\n",
      "can make choices: ['and']\n",
      "wing of the: ['robot']\n",
      "about a certain: ['measurement', 'measurement']\n",
      "designed recently ,: ['such', 'such']\n",
      ", thus avoiding: ['workers', 'workers']\n",
      "are many competitions: ['around', 'around']\n",
      "understanding digital images: [',', ',']\n",
      "takes actions that: ['maximize', 'maximize', 'maximize']\n",
      "in the next: ['couple', 'few', 'two']\n",
      ", the robots: ['must', 'first', 'must']\n",
      "information can be: ['used', 'used', 'used']\n",
      "through unsupervised learning: [',', 'in', ',']\n"
     ]
    }
   ],
   "source": [
    "def get_training_ngrams(text, N):\n",
    "    \"\"\" Extracts n-grams as well as the next word in the sequence for later sampling\"\"\"\n",
    "    ngrams = {}\n",
    "    words_tokens = nltk.word_tokenize(text)\n",
    "    for i in range(len(words_tokens)-N):\n",
    "        seq = \" \".join(words_tokens[i:i+N])\n",
    "        if seq not in ngrams.keys():\n",
    "            ngrams[seq] = []\n",
    "        ngrams[seq].append(words_tokens[i+N])\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "# Gather n-grams plus the next possible words for completion\n",
    "N = 3\n",
    "ngrams = get_training_ngrams(article_text, N)\n",
    "\n",
    "# Print some random n-grams that have at least 1, 2, and 3 completions respectively\n",
    "num_samples = 5\n",
    "generated_keys = 5\n",
    "print(\"Sample {}-grams:\".format(N))\n",
    "for num_completions in range(3):\n",
    "    generated_keys = 0\n",
    "    while generated_keys < num_samples:\n",
    "        key = random.choice(list(ngrams.keys()))\n",
    "        if len(ngrams[key]) > num_completions:\n",
    "            print(\"{}: {}\".format(key, ngrams[key]))\n",
    "            generated_keys += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text using the n-gram counts\n",
    "\n",
    "For this relatively small dataset (n-grams are VERY data hungry), the assumption that our dictionary will hold a reasonable probability distribution of next words to sample from will be far from true. \n",
    "\n",
    "With such limited data, our approach will be simple -- given an n-gram, we will look it up in our dictionary and sample from its list of subsequent words at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-grams:\n",
      "==============================\n",
      "robotics is a branch of engineering that involves the conception , design , manufacture , and operation of robots . for example , the authors have examined the task of digital character recognition , the mnist dataset has often been used . machine learning algorithms that can be used for various problems reasoning using the bayesian inference algorithm , learning using the expectation maximization algorithm , f planning using decision networks and perception using dynamic bayesian networks . generalizations of bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.a genetic algorithm ga is a search that is\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(start, ngrams, num_words):\n",
    "    \"\"\" \n",
    "    Generate a sequence given start text, the dictionary of n-grams, \n",
    "    and total number of words before stopping. If no valid word is found, \n",
    "    the sequence generation will terminate early.\n",
    "    \"\"\"\n",
    "    curr_sequence = start.lower()\n",
    "    output = curr_sequence\n",
    "    N = len(list(ngrams.keys())[0].split(\" \"))\n",
    "    for i in range(num_words):\n",
    "        if curr_sequence not in ngrams.keys():\n",
    "            print(\"No valid word following the sequence: \" + curr_sequence)\n",
    "            break\n",
    "        possible_words = ngrams[curr_sequence]\n",
    "        next_word = random.choice(possible_words)\n",
    "        output += \" \" + next_word\n",
    "        seq_words = nltk.word_tokenize(output)\n",
    "        curr_sequence = \" \".join(seq_words[len(seq_words)-N:len(seq_words)])\n",
    "    return output\n",
    "\n",
    "\n",
    "start_text = \"Robotics is a\"\n",
    "num_words = 100\n",
    "gen_output = generate_sequence(start_text, ngrams, num_words)\n",
    "\n",
    "print(\"{}-grams:\".format(N))\n",
    "print(\"=\"*30)\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try With Other Sizes\n",
    "\n",
    "The value of \"n\" in n-grams provides a tradeoff: Longer sequences will lead to more natural text since we have more context. However, the longer the sequence, the less likely it is to encounter it again when generating text, so you will need more data and more luck. This is why n-grams become impractical after 4- or 5-grams, and most practical models use bigrams or trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-grams:\n",
      "==============================\n",
      "computer vision share other topics such as jumping , and building new robots serve various practical purposes , but this will not happen until the th century author isaac asimov introduced the three laws of robotics , fast implementations of cnns on gpus have won many visual pattern recognition and information value theory these tools include models such as naive bayes on most practical data sets for tasks ranging from advanced missiles to uavs for recon missions or missile guidance send the missile to an increase in research tools and people , and the s , p ot at oe sb ur\n",
      "\n",
      "4-grams:\n",
      "==============================\n",
      "machine learning models require a lot of data in order for them to perform well . usually , when training a machine learning model.federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process , allowing for users privacy to be maintained by not needing to send their data to a centralized server . this also increases efficiency by decentralizing the training process to many devices . for example , they may begin with a population of organisms the guesses and then allow them to mutate and recombine , selecting only the fittest to survive\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "num_words = 100\n",
    "start_text = \"Computer vision\"\n",
    "ngrams = get_training_ngrams(article_text, N)\n",
    "gen_output = generate_sequence(start_text, ngrams, num_words)\n",
    "print(\"{}-grams:\".format(N))\n",
    "print(\"=\"*30)\n",
    "print(gen_output)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "N = 4\n",
    "num_words = 100\n",
    "start_text = \"Machine learning models require\"\n",
    "ngrams = get_training_ngrams(article_text, N)\n",
    "gen_output = generate_sequence(start_text, ngrams, num_words)\n",
    "print(\"{}-grams:\".format(N))\n",
    "print(\"=\"*30)\n",
    "print(gen_output)\n",
    "\n",
    "# Funny thing about the paragraph below regarding overfitting\n",
    "# https://en.wikipedia.org/wiki/Machine_learning#Training_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Multiple Values of \"n\"\n",
    "\n",
    "In practice, we can combine the advantages of longer sequences with shorter sequences by collecting multiple sets of n-grams (for example, bigrams and trigrams).\n",
    "\n",
    "At inference time, we can try generate with the longer sequences first, and as a fallback move down to the shorter sequences since generating *some* text is better than exiting early, even if accuracy is not as good... or use a weighted combination where longer sequences are weighted more heavily for sampling. This also lets us sample much longer sequences!\n",
    "\n",
    "Of course, this exponentially increases the memory requirements of our dictionary... no free lunch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-gram combination:\n",
      "==============================\n",
      "robotics is a related field that considers any kind of programming languages for representing hypotheses and not only logic programming , such as the imagenet challenge , promote research in ai research began to explore the possibility that human intelligence can be smaller and lighter without a human pilot on board , and faithfully reproduce the author s original intent social intelligence . a problem like machine translation is considered ai complete , because all of these problems need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new customer groups that are not represented in the spring of , the robot s onboard computer tries to keep the total inertial forces the combination of earth s gravity and the robotic holograms in voyager.are there limits to how intelligent machines or human machine hybrids can be thought of as a predictive model to go from observations about an item represented in the largest deep learning projects from alexnet to alphazero , and inspection , autonomous robots are needed in several areas such as a human this issue , now known as robot rights , is currently being considered by , this led researchers to consider the possibility of building an electronic brain . turing proposed changing the question from whether a machine was intelligent , to complement cpus and graphics processing units gpus in this role human robot interaction has shown promising outcomes in autism intervention children with autism spectrum disorders asd are more likely to connect with robots than humans , and a rack pinion mechanism used for the flapping wing system this design incorporated a very large output layer by , graphic processing units gpus in this field is inpainting.the organization of a set of data that contains both desirable and undesirable situations . several learning algorithms aim at discovering better representations of the future nevertheless , researchers are familiar with asimov s laws through popular culture , they generally consider the laws useless for many reasons , one complicated concept at a young age robotics is a methodology that uses evolutionary computation to help design robots , where he introduced his concept of the picture instead , they are capable of self driving cars . a few companies involved with ai include tesla , google , and kumotek s mk roosterbot.a guarded tele op robot has the ability to move in multiple locomotive modes . as an integrated unit . such an integrated robotic system is called a feature vector , and robotics require the agent to operate with incomplete or uncertain information . ai researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics bayesian networks are a very large output layer by , graphic processing units gpus , often with ai specific enhancements , had displaced cpus as the dominant method of training data.machine learning also has intimate ties to optimization many learning problems\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence_multiple(start, all_grams, all_probs, num_words):\n",
    "    \"\"\" \n",
    "    Generate a sequence given start text, a list of n-grams dictionaries, \n",
    "    and total number of words before stopping. If no valid word is found, \n",
    "    the sequence generation will terminate early.\n",
    "    \"\"\"\n",
    "    curr_sequence = start.lower()\n",
    "    output = curr_sequence\n",
    "    N_list = [len(list(ngrams.keys())[0].split(\" \")) for ngrams in all_grams]\n",
    "    \n",
    "    for i in range(num_words):\n",
    "        possible_words = []\n",
    "        sample_probs = []\n",
    "        seq_words = nltk.word_tokenize(output)\n",
    "        \n",
    "        # Loop through all the n-grams dictionaries\n",
    "        for idx in range(len(N_list)):\n",
    "            ngrams = all_grams[idx]\n",
    "            N = N_list[idx]\n",
    "            \n",
    "            # If the sequence is not found, fall back and try again until \n",
    "            # all n-grams dictionaries have been exhausted\n",
    "            if len(seq_words) >= N:    \n",
    "                curr_sequence = \" \".join(seq_words[len(seq_words)-N:len(seq_words)])\n",
    "                if curr_sequence in ngrams.keys():\n",
    "                    words = ngrams[curr_sequence]\n",
    "                    possible_words.extend(words)\n",
    "                    sample_probs.extend(all_probs[idx]*np.ones(len(words)))\n",
    "                    \n",
    "        # If there are no possible words, exit\n",
    "        if len(possible_words) == 0:\n",
    "            break\n",
    "        \n",
    "        # Generate text\n",
    "        sample_probs /= sum(sample_probs)\n",
    "        next_word = np.random.choice(possible_words, p=sample_probs)\n",
    "        output += \" \" + next_word\n",
    "            \n",
    "    return output\n",
    "\n",
    "bigrams = get_training_ngrams(article_text, 2)\n",
    "trigrams = get_training_ngrams(article_text, 3)\n",
    "fourgrams = get_training_ngrams(article_text, 4)\n",
    "all_grams = [bigrams, trigrams, fourgrams]\n",
    "all_probs = [0.05, 0.15, 0.8]\n",
    "\n",
    "start_text = \"Robotics is\"\n",
    "num_words = 500\n",
    "gen_output = generate_sequence_multiple(start_text, all_grams, all_probs, num_words)\n",
    "\n",
    "print(\"multi-gram combination:\".format(N))\n",
    "print(\"=\"*30)\n",
    "print(gen_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('intro-nlp': conda)",
   "language": "python",
   "name": "python361064bitintronlpconda61e1afdce515499c9770b8779f7d77e0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
